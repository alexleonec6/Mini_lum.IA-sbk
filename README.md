üß† Mini-lum.IA: Assistente RAG (Retrieval-Augmented Generation) Local

Este projeto implementa um sistema de Q&A (Perguntas e Respostas) baseado na arquitetura RAG (Retrieval-Augmented Generation). Ele permite que a intelig√™ncia artificial responda perguntas complexas consultando exclusivamente uma base de conhecimento privada (seus pr√≥prios documentos). A solu√ß√£o √© constru√≠da para operar de forma totalmente local e privada, garantindo total controle sobre os dados e o LLM.

üåü Destaques do Projeto

Arquitetura RAG Completa: Implementa√ß√£o de um pipeline robusto que inclui carregamento de documentos, chunking, cria√ß√£o de embeddings e busca por similaridade sem√¢ntica.

Modelo Open Source: Utiliza o Ollama como servidor de LLM e o modelo Mistral para gera√ß√£o de respostas.

Privacidade e Autonomia: Toda a opera√ß√£o √© realizada na m√°quina local, sem depend√™ncia de APIs externas ou envio de dados sens√≠veis para a nuvem.

Interface Amig√°vel: Desenvolvido com Streamlit para uma interface de chat interativa e f√°cil de usar.

üõ†Ô∏è Tecnologias Utilizadas

Categoria

Tecnologia

Uso no Projeto

Orquestra√ß√£o RAG

LangChain

Framework principal para conectar todos os componentes e gerenciar o fluxo do chat.

Servidor LLM

Ollama

Respons√°vel por executar o LLM Mistral localmente.

LLM (Gera√ß√£o)

Mistral

Modelo de c√≥digo aberto utilizado para criar embeddings e gerar as respostas finais.

Banco Vetorial

FAISS

Usado para armazenar vetores e realizar buscas por similaridade sem√¢ntica de alta performance.

Interface Web

Streamlit

Cria√ß√£o da interface de usu√°rio e da l√≥gica de sess√£o do chat.

üöÄ Instala√ß√£o e Configura√ß√£o

1. Pr√©-requisitos

Certifique-se de ter o Python (3.10+) e o Git instalados.

2. Ollama (Servidor LLM)

Voc√™ deve ter o Ollama instalado e rodando em sua m√°quina.

Instalar o Ollama: Baixe e instale a vers√£o para o seu sistema operacional.

Baixar o Modelo Mistral: Abra um terminal e execute:

ollama pull mistral


Iniciar o Servidor: Mantenha o servidor Ollama rodando em um terminal (ou em segundo plano):

ollama serve


3. Configura√ß√£o do Projeto Python

Clone o Reposit√≥rio:

git clone [https://github.com/alexleonec6/Mini_lum.IA-sbk.git](https://github.com/alexleonec6/Mini_lum.IA-sbk.git)
cd Mini_lum.IA-sbk


Criar e Ativar o Ambiente Virtual:

python -m venv venv
.\venv\Scripts\activate  # No Windows
# source venv/bin/activate  # No Linux/macOS


Instalar Depend√™ncias:

pip install -r requirements.txt


üìö Uso

1. Adicionar Documentos

Coloque todos os seus arquivos de conhecimento (.pdf, .txt, etc.) dentro da pasta data/ na raiz do projeto.

2. Iniciar o Assistente

Com o Ollama rodando e o ambiente virtual ativado, inicie o Streamlit:

streamlit run app.py


O aplicativo ser√° aberto automaticamente no seu navegador. A primeira execu√ß√£o ir√° processar e vetorizar seus documentos, criando a base FAISS no disco (o que pode levar alguns minutos, dependendo do volume de dados).

üóÇ Estrutura do Projeto

Mini_lum.IA-sbk/
‚îú‚îÄ‚îÄ data/                       # <-- Coloque seus documentos aqui (ignorada pelo Git)
‚îú‚îÄ‚îÄ venv/                       # Ambiente virtual Python (ignorada pelo Git)
‚îú‚îÄ‚îÄ .gitignore                  # Regras para ignorar 'venv/', 'data/' e cache
‚îú‚îÄ‚îÄ requirements.txt            # Lista de depend√™ncias Python (streamlit, langchain, faiss, etc.)
‚îú‚îÄ‚îÄ app.py                      # Arquivo principal do Streamlit (Interface e l√≥gica de sess√£o)
‚îî‚îÄ‚îÄ chat_engine.py              # L√≥gica central do RAG (carregamento, indexa√ß√£o FAISS e pipeline de consulta)
