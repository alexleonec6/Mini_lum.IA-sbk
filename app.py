import streamlit as st
# Garante que a importa√ß√£o do engine est√° correta
from chat_engine import criar_chain

st.set_page_config(
    page_title="Mini Lum.IA - SBK",
    page_icon="üí°",
    layout="wide"
)

st.title("üí° Mini Lum.IA - Assistente SBK")
st.markdown("**Chat de IA local que responde perguntas sobre documentos internos**")
st.markdown("---")

with st.sidebar:
    st.header("üìä Status do Sistema")
    st.markdown("""
    **Configura√ß√£o:**
    - ü§ñ Modelo: Mistral (local, via Ollama)
    - üìö Documentos: Pasta (data)
    - üîç Busca sem√¢ntica (FAISS)
    - üåê Totalmente offline
    """)

    if st.button("üîÑ Recarregar Sistema"):
        # Limpa o cache e for√ßa a recria√ß√£o da chain
        if "chain" in st.session_state:
            del st.session_state.chain
        st.rerun()

if "chain" not in st.session_state:
    with st.spinner("üöÄ Inicializando sistema IA... Isso pode levar alguns segundos (dependendo do tamanho dos documentos)."):
        try:
            # Tenta criar e carregar a chain RAG
            st.session_state.chain = criar_chain()
            st.success("‚úÖ Sistema carregado com sucesso!")
        except Exception as e:
            # Exibe erro e dicas de solu√ß√£o de problemas
            st.error(f"‚ùå Erro de inicializa√ß√£o: {str(e)}")
            st.info("""
            **Para resolver:**
            1. Verifique se o **Ollama est√° rodando** em outro terminal: `ollama serve`
            2. Confirme se o **modelo Mistral est√° instalado**: `ollama pull mistral`
            3. Verifique se a **pasta 'data'** existe e cont√©m arquivos PDF ou TXT.
            """)

st.subheader("üí¨ Fa√ßa sua pergunta sobre os documentos")

pergunta = st.text_input(
    "Digite sua pergunta:",
    placeholder="Ex: Quais s√£o as pol√≠ticas de f√©rias? O que diz o documento X sobre o processo Y?"
)

if st.button("üîç Buscar Resposta", type="primary") and pergunta:
    if "chain" not in st.session_state:
        st.error("Sistema n√£o carregado. Verifique a se√ß√£o de status acima.")
    else:
        with st.spinner("üîç Consultando documentos e gerando resposta..."):
            try:
                # Usa .invoke() para LangChain 0.1.x
                resultado = st.session_state.chain.invoke({"query": pergunta}) 
                st.markdown("### üß† Resposta:")
                st.write(resultado["result"])

                with st.expander("üìö Ver documentos consultados"):
                    if resultado.get("source_documents"):
                        st.markdown("Estes s√£o os trechos que a IA utilizou como contexto:")
                        for i, doc in enumerate(resultado["source_documents"][:3], 1):
                            fonte = doc.metadata.get("fonte", "N/A")
                            # Usa get('page', 'N/A') se estiver usando PyPDFLoader
                            pagina = doc.metadata.get('page', 'N/A')
                            st.markdown(f"**Documento {i}:** {fonte} (P√°g: {pagina})") 
                            
                            # Limita o tamanho do preview
                            conteudo = doc.page_content.replace('\n', ' ')
                            conteudo_preview = conteudo[:250] + "..." if len(conteudo) > 250 else conteudo
                            st.text(conteudo_preview)
                            st.markdown("---")
                    else:
                        st.info("Nenhum documento espec√≠fico foi consultado (a resposta pode ser conhecimento geral do modelo).")
            except Exception as e:
                st.error(f"Erro ao processar a consulta. Verifique o console para mais detalhes. Erro: {str(e)}")

st.markdown("---")
st.markdown("üí° **Dica:** Fa√ßa perguntas espec√≠ficas sobre o conte√∫do dos documentos para melhores respostas. Lembre-se de que o **Ollama deve estar ativo** para que o sistema funcione.")
